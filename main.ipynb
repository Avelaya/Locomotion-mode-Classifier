{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7a3039",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6857d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, Adagrad, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from keras.layers import ConvLSTM2D, ConvLSTM1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from scipy.stats import mode\n",
    "import keras\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad909eb",
   "metadata": {},
   "source": [
    "# Data Precessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565e67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for oversampling\n",
    "def oversample_data(X_train, y_train):\n",
    "    # Create an instance of RandomOverSampler\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    # Apply Random Oversampling to the training set\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def shuffle_sections(df):\n",
    "    # Create a list of unique trial numbers\n",
    "    trial_numbers = df['trial_number'].unique()\n",
    "    \n",
    "    # Shuffle the order of trial numbers\n",
    "    np.random.shuffle(trial_numbers)\n",
    "    \n",
    "    # Create a list to store shuffled sections\n",
    "    shuffled_sections = []\n",
    "    \n",
    "    # Iterate over shuffled trial numbers\n",
    "    for trial_number in trial_numbers:\n",
    "        # Extract rows for the current trial number\n",
    "        section = df[df['trial_number'] == trial_number]\n",
    "        # Append the section to the shuffled sections list\n",
    "        shuffled_sections.append(section)\n",
    "    \n",
    "    # Concatenate the shuffled sections back into a DataFrame\n",
    "    shuffled_df = pd.concat(shuffled_sections)\n",
    "    \n",
    "    return shuffled_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_mode_label(labels):\n",
    "    # Calculate the mode of the labels within the sequence\n",
    "    mode_label, _ = mode(labels)\n",
    "    return mode_label\n",
    "\n",
    "\n",
    "\n",
    "def csv_to_array(initial_csv):\n",
    "    return np.squeeze(initial_csv.to_numpy(), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sld_wnd(data, window, stride):\n",
    "    result = np.lib.stride_tricks.sliding_window_view(data,window)[::stride, :]\n",
    "    return result\n",
    "\n",
    "def sld_wnd_2(data, window, stride):\n",
    "    result = np.lib.stride_tricks.sliding_window_view(data,window)[::stride, :]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to read x and y files, perform operations, and return the final dataframe\n",
    "def process_files(x_file, y_file, trial_number):\n",
    "    # Read x and y files\n",
    "    feature = pd.read_csv(x_file, header=None)\n",
    "    target = pd.read_csv(y_file, header=None)\n",
    "    \n",
    "    # Extract the index column\n",
    "    feature_t = feature.iloc[:, 0].to_frame()\n",
    "    target_t = target.iloc[:, 0].to_frame()\n",
    "    \n",
    "    # Drop the index column\n",
    "    feature = feature.drop(feature.columns[0], axis=1)\n",
    "    target = target.drop(target.columns[0], axis=1)\n",
    "    \n",
    "    # Set index using the extracted index column\n",
    "    feature.index = csv_to_array(feature_t)\n",
    "    target.index = csv_to_array(target_t)\n",
    "    \n",
    "    # Rename columns\n",
    "    feature.columns = ['x_accelerometers', 'y_accelerometers', 'z_accelerometers', 'x_gyroscope', 'y_gyroscope', 'z_gyroscope']\n",
    "    target.columns = ['target']\n",
    "    \n",
    "    # Concatenate feature and target\n",
    "    data = pd.concat([feature, target], axis=1)\n",
    "    \n",
    "    # Sort by index\n",
    "    data = data.sort_index(axis=0)\n",
    "    \n",
    "    # Interpolate target column\n",
    "    data.interpolate(method = \"linear\", inplace = True) # linear interpolation for the missing values\n",
    "    data.target = np.round(data.target)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    #data = data.dropna()\n",
    "    data.dropna(axis = 'index', inplace = True)\n",
    "    \n",
    "    # Add a column for trial number\n",
    "    data['trial_number'] = trial_number\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to read x and y files, perform operations, and return the final dataframe\n",
    "def process_files_test(feature, target):\n",
    "\n",
    "    # Extract the index column\n",
    "    feature_t = feature.iloc[:, 0].to_frame()\n",
    "    target_t = target.iloc[:, 0].to_frame()\n",
    "    \n",
    "    # Drop the index column\n",
    "    feature = feature.drop(feature.columns[0], axis=1)\n",
    "    target = target.drop(target.columns[0], axis=1)\n",
    "    \n",
    "    # Set index using the extracted index column\n",
    "    feature.index = csv_to_array(feature_t)\n",
    "    target.index = csv_to_array(target_t)\n",
    "    \n",
    "    # Rename columns\n",
    "    feature.columns = ['x_accelerometers', 'y_accelerometers', 'z_accelerometers', 'x_gyroscope', 'y_gyroscope', 'z_gyroscope']\n",
    "    target.columns = ['target']\n",
    "    \n",
    "    # Concatenate feature and target\n",
    "    data = pd.concat([feature, target], axis=1)\n",
    "    \n",
    "    # Sort by index\n",
    "    data = data.sort_index(axis=0)\n",
    "    \n",
    "    # Interpolate target column\n",
    "    data.interpolate(method = \"linear\", inplace = True) # linear interpolation for the missing values\n",
    "    data.target = np.round(data.target)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    #data = data.dropna()\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964273a",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the final result\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "# Loop over x files\n",
    "for i in range(1, 30):\n",
    "    x_file = f\"Trial{i:02d}_x.csv\"\n",
    "    y_file = f\"Trial{i:02d}_y.csv\"\n",
    "    \n",
    "    # Check if both x and y files exist\n",
    "    if os.path.exists(x_file) and os.path.exists(y_file):\n",
    "        # Process the files and concatenate the result with the final_data\n",
    "        result = process_files(x_file, y_file, i)\n",
    "        final_data = pd.concat([final_data, result])\n",
    "        \n",
    "                \n",
    "final_data = shuffle_sections(final_data)\n",
    "\n",
    "column_target = final_data.pop('target')\n",
    "\n",
    "# Append the dropped column to the end of the DataFrame\n",
    "final_data['target'] = column_target\n",
    "\n",
    "print(final_data['target'].value_counts(normalize=True) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85961e0",
   "metadata": {},
   "source": [
    "# Splitting the Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c273692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train, validation, and test data sets for each trial number, getting 80% of the data for training and 10% each for validation and test\n",
    "\n",
    "train_X = []\n",
    "val_X = []\n",
    "test_X = []\n",
    "train_Y = []\n",
    "val_Y = []\n",
    "test_Y = []\n",
    "\n",
    "# Iterate over unique trial numbers in the dataset\n",
    "for trial_number in pd.unique(final_data.loc[:, \"trial_number\"]):\n",
    "    # Split data for the current trial number into training, validation, and test sets\n",
    "    train_x, remaining_x, train_y, remaining_y = train_test_split(\n",
    "        final_data[final_data[\"trial_number\"] == trial_number].loc[:, \"x_accelerometers\":\"z_gyroscope\"], \n",
    "        final_data[final_data[\"trial_number\"] == trial_number].loc[:, \"target\"], \n",
    "        test_size=0.20, shuffle=False)\n",
    "    \n",
    "    val_x, test_x, val_y, test_y = train_test_split(\n",
    "        remaining_x, remaining_y, test_size=0.50, shuffle=False)\n",
    "    \n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    train_x_resampled, train_y_resampled = oversampler.fit_resample(train_x, train_y)\n",
    "    \n",
    "    \n",
    "    # Append the training, validation, and test data for features and labels to respective lists\n",
    "    train_X.append(train_x_resampled)\n",
    "    val_X.append(val_x)\n",
    "    test_X.append(test_x)\n",
    "    train_Y.append(train_y_resampled)\n",
    "    val_Y.append(val_y)\n",
    "    test_Y.append(test_y)\n",
    "    \n",
    "# Concatenate the lists of training, validation, and test data to create single dataframes\n",
    "train_X = pd.concat(train_X)\n",
    "val_X = pd.concat(val_X)\n",
    "test_X = pd.concat(test_X)\n",
    "train_Y = pd.concat(train_Y)\n",
    "val_Y = pd.concat(val_Y)\n",
    "test_Y = pd.concat(test_Y)\n",
    "\n",
    "# Concatenate features and labels to create train, validation, and test data\n",
    "train_data = pd.concat([train_X, train_Y], axis=1)\n",
    "val_data = pd.concat([val_X, val_Y], axis=1)\n",
    "test_data = pd.concat([test_X, test_Y], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d9d0f",
   "metadata": {},
   "source": [
    "# Generating Sequences of Data Followed by Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 64\n",
    "step = 1\n",
    "\n",
    "# Apply sliding window transformation to training features\n",
    "train_X = sld_wnd_2(train_X, (window_size, 6), step)\n",
    "\n",
    "# Apply sliding window transformation to validation features\n",
    "val_X = sld_wnd_2(val_X, (window_size, 6), step)\n",
    "\n",
    "# Apply sliding window transformation to test features\n",
    "test_X = sld_wnd_2(test_X, (window_size, 6), step)\n",
    "\n",
    "# Apply sliding window transformation to training labels\n",
    "train_Y = sld_wnd(train_Y, window_size, step)\n",
    "\n",
    "# Apply sliding window transformation to validation labels\n",
    "val_Y = sld_wnd(val_Y, window_size, step)\n",
    "\n",
    "# Apply sliding window transformation to test labels\n",
    "test_Y = sld_wnd(test_Y, window_size, step)\n",
    "\n",
    "\n",
    "train_Y_mode = [extract_mode_label(train_Y[i]) for i in range(train_Y.shape[0])]\n",
    "train_Y = np.array(train_Y_mode)\n",
    "\n",
    "# Extract mode label for validation set\n",
    "val_Y_mode = [extract_mode_label(val_Y[i]) for i in range(val_Y.shape[0])]\n",
    "val_Y = np.array(val_Y_mode)\n",
    "\n",
    "# Extract mode label for test set\n",
    "test_Y_mode = [extract_mode_label(test_Y[i]) for i in range(test_Y.shape[0])]\n",
    "test_Y = np.array(test_Y_mode)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape training features\n",
    "train_X = np.reshape(train_X, (len(train_X), window_size, 6))\n",
    "\n",
    "# Reshape validation features\n",
    "val_X = np.reshape(val_X, (len(val_X), window_size, 6))\n",
    "\n",
    "# Reshape test features\n",
    "test_X = np.reshape(test_X, (len(test_X), window_size, 6))\n",
    "\n",
    "# Reshape training labels\n",
    "train_Y = np.reshape(train_Y, (len(train_Y), 1))\n",
    "\n",
    "# Reshape validation labels\n",
    "val_Y = np.reshape(val_Y, (len(val_Y), 1))\n",
    "\n",
    "# Reshape test labels\n",
    "test_Y = np.reshape(test_Y, (len(test_Y), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape train_X to 2D array\n",
    "train_X_flattened = train_X.reshape(-1, train_X.shape[-1])\n",
    "\n",
    "# Reshape val_X to 2D array\n",
    "val_X_flattened = val_X.reshape(-1, val_X.shape[-1])\n",
    "\n",
    "# Reshape test_X to 2D array\n",
    "test_X_flattened = test_X.reshape(-1, test_X.shape[-1])\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform the flattened data\n",
    "train_X_scaled_flattened = scaler.fit_transform(train_X_flattened)\n",
    "\n",
    "# Transform the flattened validation and test data using the scaler fitted on training data\n",
    "val_X_scaled_flattened = scaler.transform(val_X_flattened)\n",
    "test_X_scaled_flattened = scaler.transform(test_X_flattened)\n",
    "\n",
    "# Reshape the scaled data back to 3D array for train, val, and test\n",
    "train_X_scaled = train_X_scaled_flattened.reshape(train_X.shape)\n",
    "val_X_scaled = val_X_scaled_flattened.reshape(val_X.shape)\n",
    "test_X_scaled = test_X_scaled_flattened.reshape(test_X.shape)\n",
    "\n",
    "#X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "#X_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)\n",
    "#X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bb8a2",
   "metadata": {},
   "source": [
    "# Model Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74107505",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = train_X.shape[1]\n",
    "n_features = train_X.shape[2]\n",
    "n_outputs = train_Y.shape[1]\n",
    "n_steps = 4\n",
    "n_length = 16\n",
    "train_X_scaled = train_X_scaled.reshape((train_X_scaled.shape[0], n_steps, 1, n_length, n_features))\n",
    "val_X_scaled = val_X_scaled.reshape((val_X_scaled.shape[0], n_steps, 1, n_length, n_features))\n",
    "test_X_scaled = test_X_scaled.reshape((test_X_scaled.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "#train_X_scaled_copy = train_X_scaled_copy.reshape((train_X_scaled_copy.shape[0], n_steps, 1, n_length, n_features))\n",
    "#val_X_scaled_copy = val_X_scaled_copy.reshape((val_X_scaled_copy.shape[0], n_steps, 1, n_length, n_features))\n",
    "#test_X_scaled_copy = test_X_scaled_copy.reshape((test_X_scaled_copy.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters = 64, kernel_size = (1,3), input_shape = (n_steps, 1, n_length, n_features)))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(75))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 5, restore_best_weights = True)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_X_scaled, train_Y, epochs=20, batch_size=500, validation_data=(val_X_scaled, val_Y), verbose = 1, callbacks = [early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391c6d8",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c80ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_X_scaled, test_Y)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ab0b8",
   "metadata": {},
   "source": [
    "# Unseen data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):  # Iterate from 1 to 4\n",
    "    # Load test set data\n",
    "    test_x_filename = f\"Test0{i}_x.csv\"\n",
    "    test_y_filename = f\"Test0{i}_y.csv\"\n",
    "    test_x_path = os.path.dirname(test_x_filename)\n",
    "    test_x = pd.read_csv(test_x_filename, header=None)\n",
    "    test_y = pd.read_csv(test_y_filename, header=None)\n",
    "    \n",
    "    data2 = process_files_test(test_x, test_y)\n",
    "    X_test01 = data2.iloc[:, 0:6].values\n",
    "    \n",
    "    X_test01 = sld_wnd_2(X_test01, (window_size, 6), step)\n",
    "    X_test01 = np.reshape(X_test01, (len(X_test01), window_size, 6))\n",
    "    X_test01_flattened = X_test01.reshape(-1, X_test01.shape[-1])\n",
    "    X_test01_scaled_flattened = scaler.transform(X_test01_flattened)\n",
    "    X_test01_scaled= X_test01_scaled_flattened.reshape(X_test01.shape)\n",
    "    X_test01_scaled = X_test01_scaled.reshape((X_test01_scaled.shape[0], n_steps, 1, n_length, n_features))\n",
    "    predictions = model.predict(X_test01_scaled, batch_size=3000)\n",
    "    \n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    data2.reset_index(drop=False, inplace=True)\n",
    "    data2['target'][:len(predicted_classes)] = predicted_classes\n",
    "    data2['target'] = data2['target'].fillna(method='ffill')\n",
    "    \n",
    "    \n",
    "    time_column_name = \"time\"  # Replace this with the actual name of your time column\n",
    "    data2 = data2.rename(columns={data2.columns[0]: time_column_name})\n",
    "    test_y = test_y.rename(columns={test_y.columns[0]: time_column_name})\n",
    "\n",
    "    # Merge the two dataframes on the time column to find common rows\n",
    "    merged_df = pd.merge(data2, test_y, on=time_column_name, how='inner')\n",
    "\n",
    "    # Now, select only the time and label columns from df1\n",
    "    new_df = merged_df[[time_column_name, 'target']]  # Replace 'label' with the actual name of your label column in df1\n",
    "\n",
    "    # Write the resulting DataFrame to a CSV file in the same directory\n",
    "    output_filename = f\"result_test0{i}.csv\"\n",
    "    output_file_path = os.path.join(test_x_path, output_filename)\n",
    "    new_df.to_csv(output_file_path, header=False, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
